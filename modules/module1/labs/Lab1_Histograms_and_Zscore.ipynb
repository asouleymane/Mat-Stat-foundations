{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Reviewing Statistics and R\n",
    "\n",
    "This is your first lab for the Introduction to Statistical and Mathematical Foundations of Data Science course. You can refer to chapters 1 to 3 in [intro to statistics textbook](http://onlinestatbook.com/2/index.html) book for reference. \n",
    "\n",
    "We will look into basics of statistics mainly for univariate data analysis. Some of the basic concepts like descriptive and inferential statistics, distributions, graphing and summarizing distributions using measures of central tendency and variability are explained through R commands. \n",
    "\n",
    "Some of the concepts may have been covered in Intro to Data Science course. We will refresh those concepts here a little bit. We will look into the miles per gallon dataset in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loading data\n",
    "\n",
    "Load the data `auto-mpg` into R and view the first few rows (including the column names (called the header)) in the following way. This data is about city-cycle fuel consumption based on different types of cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "auto_mpg <- read.csv(\"../../../datasets/auto-mpg/auto-mpg.csv\", header = T, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can get a quick view of some of the relavent information about the data set `auto_mpg` using head() function. **`head()`** shows the first five rows by default (including the header) if no argument is provided.   Just typing the R variable `auto_mpg` will return all the rows in the table which is time consuming if you are dealing with a very big dataset. Head() is really handy when you just want to see how the data looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(auto_mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**names() ** : The auto_mpg dataset has the column names already set. But in many cases, a dataset will not have headers for the columns or the names need some formatting. names() function is helpful in manipulating the names of datframe columns. It is illustrated below how you can use this command. \n",
    "\n",
    "\n",
    "`Usage: names(x) <- value`\n",
    "\n",
    "\n",
    "`It` shows the variable (i.e. column) names of the input dataframe `x`. You can also manipulate column names using this command. You can assign the variable names as a vector of names. If the length of character vector of names is less than number of variables in the dataframe, it is extended by character NAs to the length of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names(auto_mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assign names of auto_mpg columns to the variable 'column_names'\n",
    "column_names=names(auto_mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Modify the names of columns of auto_mpg dataset by assigning new names.\n",
    "names(auto_mpg)=c('a','b','c','d','e','f','g','h','i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing a specific column name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Modify the name of first column of auto_mpg dataset.\n",
    "names(auto_mpg)[1]='z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Display the names of auto_mpg columns\n",
    "names(auto_mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Assigning the original column names back to auto_mpg dataset variables.\n",
    "names(auto_mpg)=column_names\n",
    "names(auto_mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "**`summary()`** \n",
    "\n",
    "Summary() command gives a summary of each variable in the dataframe. As shown below, the command is very informative. It gives minimum value, 1st quartile, 2nd quartile(median), mean value, 3rd quartile, and maximum values of numeric variables. If the variable has NA values, number of such rows with NA values is displayed too. \n",
    "\n",
    "You can use this information to quickly classify the variables into qualitative or quantitative(discrete or continue). For example all the variables in auto_mpg dataset except for origin and car.name are continuous where as origin is discrete and car.name is qualitative(nominal) variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(auto_mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you wonder why there is no min,max,mean or other values given for horsepower variable. Same is the case with car.name but it makes sense for car.name varaible. It cant have minimum value or mean values as it has strings in it. You might wanna run str() function to understand the difference between horsepower and other numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(auto_mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horsepower belongs to class or datatype `factor`. Factors are categorical variables. They are not treated as continuous variables. So the summary function couldn't calculate mean, median, min etc values for Horsepower variable. \n",
    "\n",
    "str() function tells you the datatype of variables, the dimensions of the dataframe and also an overview of kind of values each variable contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Access in R\n",
    "\n",
    "Recall, that column access in R is accomplished by using the __ \\$ __ operator.  \n",
    "```R\n",
    "dataframe$columnName\n",
    "```\n",
    "\n",
    "**`summary(auto$weight)`** shows summary of `weight` variable of the `auto_mpg` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(auto_mpg$weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Descriptive Statistics\n",
    "\n",
    "Descriptive statistics are used to summarize and describe data. If we are analyzing miles per gallon data, for example, a descriptive statistic might be the percentage of cars with different number of cylinders , or the average miles per gallon for all cars. Many descriptive statistics are often used at one time to give a full picture of the data. \n",
    "\n",
    "Descriptive statistics are just descriptive. They cannot generalize anything beyond the data at hand. Generalizing from our data to another set of cases is dealt in inferential statistics. \n",
    "\n",
    "Descriptive statistics uses measures of Central Tendency and Spread to describe the data. R has built in functions to calculate mean,median,standard deviation etc which comes under measures of central tendency and spread.\n",
    "\n",
    "These terms should be familiar from week 2 of the introductory data sciecne course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets run the head() command on auto_mpg to peek into the dataset.\n",
    "head(auto_mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Mean\n",
    "Mean gives the average value of the column/variable. Mean is the most basic and important statistic which tries to answers many questions like average mpg for a particular cylinder vehicles, helps in understanding distribution of data of a variable some times etc.\n",
    "    \n",
    "    Mean = Sum of all Observations / No. of Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#paste function below is used to concatenate strings to make things readable. paste concatenates strings in the order they \n",
    "#are given as input. \n",
    "paste('Average auto displacement is: ',mean(auto_mpg$displacement))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On an average each vehicle has a displacement of 193.42. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode\n",
    "    \n",
    "    Mode = Most occuring value in a set of values, i.e., a column of a dataframe\n",
    "    \n",
    "Mode is the value that has been repeated most frequently in a set of values. R does not have any built-in function to find out Mode. However, the mode() function returns the type or storage mode of the object instead. There are ways you can compute the _mode_(dataframe\\$columnName). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paste('Datatype of mpg is ',mode(auto_mpg$mpg))\n",
    "\n",
    "# You can use a command like below to get the most frequently occuring value. \n",
    "# Table() command tells us distribution/count of different values of a variable. \n",
    "# So in below command we are printing the value which is maximum of table\n",
    "#  command output. Simply put, maximum of count of values repeated. \n",
    "# Lets check how it works first by showing distribution of values in mpg variable and then use it to find the mode\n",
    "table(auto_mpg$mpg)\n",
    "\n",
    "#Find mode of mpg variable\n",
    "paste(\"mode using which.max(): \",names(which.max(table(auto_mpg$mpg))))\n",
    "\n",
    "\n",
    "#Or you can do as shown below which is complicated. \n",
    "actual_mode=table(auto_mpg$mpg)\n",
    "paste('Most common value for miles per gallon is: ',names(actual_mode)[actual_mode == max(actual_mode)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the output above, the table is line-wrapping in the display\n",
    "```\n",
    "Value->   9   10   11   12   13   14 14.5   15 15.5   16 16.2 16.5 16.9   17 17.5 17.6 \n",
    "Count->   1    2    4    6   20   19    1   16    5   13    1    3    1    7    5    2 \n",
    "\n",
    "Value->   17.7   18 18.1 18.2 18.5 18.6   19 19.1 19.2 19.4 19.8 19.9   20 20.2 20.3 20.5\n",
    "Count->      1   17    2    1    3    1   12    1    3    2    1    1    9    4    1    3\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "We see that 13 MPG occurs 20 times.\n",
    "\n",
    "**Note:** What table() is actually doing is computing a histogram of the value for the given set / column.\n",
    "\n",
    "This tells us, 13 is the most commonly occuring mileage(miles per gallon) of the vehicles. Of the 398 vehicles 20 vehicles have 13 miles per gallon. 13 was at position 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median\n",
    "    \n",
    "    Median = Mid point of all values. \n",
    "\n",
    "Median value divides the data set into two equal halves. One half lies to the left of median and the other to the right. Median values are less affected by outliers compared to mean. Therefore median is considered as ideal choice for measuring central tendency when the data is skewed. \n",
    "\n",
    "R has builtin function to calculate median. Lets calculate the median for displacement variable in auto_mpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paste(\"median: \",median(auto_mpg$acceleration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 398 observations in the dataset, 199 observations have acceleration less than or equal to 15.5 and other 199 observations have acceleration greater than or equal to 15.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range\n",
    "    \n",
    "    Range = (lowest value,highest value)\n",
    "    \n",
    "Range is a measure of spread, how values are spread in the variable. You will get to know the amount of variation in the data. R has builtin function to calculate range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range(auto_mpg$model.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dataset has cars with the model numbers starting from year 70 to year 82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantile\n",
    "\n",
    "Quantile function divides the data set into 4 equal parts as first Quantile(Q1), Second Quantile(Q2), Third Quantile(Q3) and fourth quantile(Q4).\n",
    "\n",
    "Quantiles are well understood when used with box plots. Box plots summarize and identify minimum, maximum, Q1, Q2 and Q3 values of a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quantile(auto_mpg$displacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command is very informative as it gives minimum, maximum, 25th percentile, 50th percentile(median) and 75th percentile values of the variable. Quantile is used for explaining the variance in the variable as it is less immune to outliers and explains variation better than other variability measures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance\n",
    "\n",
    "    The average of the squared differences from the Mean.\n",
    "    \n",
    "It measures how widely the values in a variable vary. If the observations vary greatly from the variable mean, the variance will be big and vice versa. R has builtin function to calculate the variation so that we dont have to get into the math of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paste('variance: ',var(auto_mpg$displacement))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above value represents the squared error of all the displacement values. Variance doesnt make much sense when trying to understand the spread of the data. Standard deviation will give us an idea how data is spread. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard deviation\n",
    "\n",
    "    SQRT(variance) = Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paste(\"standard deviation: \",sd(auto_mpg$displacement))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in displacement variable, vary by deviation of 104.27 with the mean displacement of 193.42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statisical inference, there are basically two treatments of variables in a dataset, **Dependent** and **Independent** variables. The independent variables are used to predict the dependent variable's outcome. For example, in our auto-mpg dataset the variable 'mpg' can be dependent, i.e., it can be predicted other variables in the dataset.  \n",
    "\n",
    "The variables often share a correlation among themselves. For example, the displacement of a vehicle might be correlated to mpg. Small vehicles tend to have more miles per gallon compared to big vehicles with low miles per gallon. A correlation between a dependent variable and an independent variable doesn't mean that independent variable is causing the changes in dependent variable. Its just dependent variables changes according to independent variable. \n",
    "\n",
    "### Types of Variables:\n",
    "\n",
    "Most important distinction between variables is if they are either qualitative or quantitative. \n",
    "\n",
    "**Qualitative variables:** Variables that express a qualitative attribute such as religion, favorite movie, gender, and so on fall into this category. They are sometimes referred to as categorical variables. \n",
    "\n",
    "**Quantitative variables:** Variables that are measured in terms of numbers. Some examples are height, weight, and shoe size.\n",
    "\n",
    "Generally speaking, when the variable has a numeric value, we call it quantitative data. When you classify something, we call it qualitative data.\n",
    "\n",
    "##### Flavous of Quantitative data:\n",
    "\n",
    "**Discrete variables: ** Data is discrete and can't be made more precise. For example, number of children in a family is discrete, because you are counting indivisible entities. You can't have 2.5 kids or 1.3 pets.\n",
    "\n",
    "**Continuous data: ** Data can be reduced to finer levels or we can say it is continuous in nature. For example, you can measure the weight of yourself at different precise scales—kilograms, grams, milligrams and beyond. So weight is continuous data.\n",
    "\n",
    "\n",
    "### Levels of measurment\n",
    "\n",
    "Both qualitative and quantitative variables follow levels of measurment. \n",
    "There are 4 levels namely nominal, ordinal, interval and ratio scaled. \n",
    "\n",
    "**nominal:** Exmaple : car.name - since it just has car names has its values. \n",
    "More examples could be marital status, gender, religion affliation etc.\n",
    "\n",
    "**ordinal:** Exmaple : cylinders - increase in the number of cylinders in a car mean higher horsepower. \n",
    "Number of cylinders in car have an order. \n",
    "More examples could be ranking of soldiers, grade a student belongs to, etc.\n",
    "\n",
    "**interval: ** We can classify acceleration as interval scaled variable. \n",
    "As each interval for eg, 10-11, 11-12 makes sense. \n",
    "More examples could be Temperature, IQ level etc. \n",
    "\n",
    "**ratio:** We dont have a ratio scaled variable in the auto_mpg dataset. \n",
    "An example for ratio scaled variable could be daily calorie intake, GPA score. \n",
    "These values will make sense when you take a ratio with respect to something else like a students GPA compared to highest GPA in the class. \n",
    "\n",
    "These levels are explained in further detail in the [statistics online text book](http://onlinestatbook.com/2/introduction/levels_of_measurement.html).We will explore the concepts with the  _auto_mpg_ dataset columns and classify them into different kinds of varables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(auto_mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above summary of data we can identify what kind of variable each one is\n",
    "\n",
    "mpg - quantitative[continuous] : \n",
    "\n",
    "      mpg data is numeric and continuous in nature.\n",
    "\n",
    "cylinders - quantitative[discrete] : \n",
    "\n",
    "      cylinders data is numeric and its discrete in nature with levels 3,4,5,6,8.\n",
    "\n",
    "displacement - quantitative[continuous] : \n",
    "\n",
    "      Displacement data is numeric and continuous in nature.\n",
    "\n",
    "horsepower - quantitative[continuous] : \n",
    "\n",
    "      Horsepower data is numeric and continuous in nature.\n",
    "\n",
    "weight - quantitative[continuous] : \n",
    "\n",
    "      Weight data is numeric and continuous in nature.\n",
    "\n",
    "acceleration - quantitative[continuous] : \n",
    "\n",
    "      Acceleration data is numeric and continuous in nature.\n",
    "\n",
    "model.year - quantitative[discrete] : \n",
    "\n",
    "      model.year data is numeric but is discrete with values ranging from 70 through 82.\n",
    "\n",
    "origin - quantitative[discrete] : \n",
    "\n",
    "      origin data is numeric and its discrete in nature with levels 1,2,3.\n",
    "\n",
    "car.name - qualitative[nominal] : \n",
    "\n",
    "      Car names are nominal. So it is qualitative data. \n",
    "\n",
    "For variables, origin, model.year and cylinders the summary() doesn't give full information to classify them as continuous or discrete. You should explore the data and see the values in dataset to classify them as discrete.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferential Statistics\n",
    "\n",
    "Inferential statistic measures helps us draw inferences about larger population from sample data. \n",
    "We try to gain insights from sample and generalize them on larger population. \n",
    "Let’s find out the inference which we can draw from _bdims_ data set we are going to work on. \n",
    "The dataset contains body dimensions data from 247 men and 260 women. \n",
    "\n",
    "Let’s say, we want to check the significance of variable sex for hypothesis testing.\n",
    "Assume that males(sex 1) have more weight than average population weight.\n",
    "\n",
    "To verify this assumption, let’s use z-test and see, if males are actually heavier than the over all population.\n",
    "\n",
    "\n",
    "H0: There is no significant difference in the weights of men and women\n",
    "\n",
    "H1: There is a better chance of men being heavier than average population weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download.file(\"http://www.openintro.org/stat/data/bdims.RData\", destfile = \"bdims.RData\")\n",
    "load(\"bdims.RData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick peek into first few rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(bdims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result you see that every observation has 25 measurements. The variable names description can be found at http://www.openintro.org/stat/data/bdims.php. We will work with just two columns for now: weight in kg (wgt) and sex (1 indicates male, 0 indicates female).\n",
    "\n",
    "Lets go ahead and create two different data sets: for men and women as both have different body dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "male <- subset(bdims, sex == 1)\n",
    "female <- subset(bdims, sex == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating Z-score\n",
    "\n",
    "A z-score is a measure of how many standard deviations below or above the population mean an observation is or the number of standard deviations from the mean a data point is. Mathematically it is calculated as  \n",
    "\n",
    "$$z = (x - \\mu)/\\sigma$$\n",
    "\n",
    "where x = sample mean,\n",
    "      $\\mu$ = population mean and  \n",
    "      $\\sigma$ = population standard deviation  \n",
    "      \n",
    "Recall from the Introduction to Data Science \\& Analytics course, the descriptive statistics and dispersion concept vidoes. Variance is the average difference squared from the mean; and standard deviation is then the square root of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \n",
    " sample_mean = mean(male$wgt)\n",
    " pop_mean = mean(bdims$wgt)\n",
    " pop_var = var(bdims$wgt)\n",
    " print(paste(\"sample mean : \",sample_mean))\n",
    " print(paste(\"population mean : \",pop_mean))\n",
    " print(paste(\"population variance : \",pop_var))\n",
    " zscore = (sample_mean - pop_mean) / (sqrt(pop_var))\n",
    " print(paste(\"Z-score : \",zscore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Below function is to calculate z score\n",
    "#   You can write function since R is just like any other programming language. \n",
    "#   The function evaluates mean of sample and population, standard deviation \n",
    "#    of population and calculates z score.\n",
    "z.score = function(sam, pop){\n",
    " sample_mean = mean(sam)\n",
    " pop_mean = mean(pop)\n",
    " pop_var = var(pop)\n",
    " zscore = (sample_mean - pop_mean) / (sqrt(pop_var))\n",
    " return(zscore)\n",
    "}\n",
    "\n",
    "#call function\n",
    "#    WE are passing male weight in sample, population weight \n",
    "z.score(male$wgt, bdims$wgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z score is 0.67 after rounding it to 2 decimals. \n",
    "Now we need to work out the percentage (or number) of men that weigh higher and lower than the population mean. \n",
    "We refer to the standard normal distribution table to find out this percentage value. \n",
    "\n",
    "![Standard Normal Distribution Table](../images/normal-table-large.png)\n",
    "\n",
    "To read the table, we break our z-score into two parts 0.67 = 0.6 (_tenths_) + 0.07 (_hundreths_)\n",
    "\n",
    "The tenths component is used to find the appropriate row in the table.  \n",
    "The hundreths component is used for the column. \n",
    "You then find the cell in the table for that row and column, and this represents % of population with a smaller value.\n",
    "\n",
    "Using the table, we can see that 74.86% of the population is lower than the average weight of men.\n",
    "\n",
    "This affirms our hypothesis, H1 above, that males tends to have more weight than population weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_mean = mean(male$wgt)\n",
    "pop_mean = mean(bdims$wgt)\n",
    "pop_sd = sd(bdims$wgt)\n",
    "\n",
    "# Instead of a table, use R\n",
    "# probability under normal distribution for sample measure, mean, standard deviation.\n",
    "p = pnorm(sample_mean, mean=pop_mean, sd=pop_sd, lower.tail=TRUE) \n",
    "\n",
    "# Since p is a probability, we can confer this to a percentage;  \n",
    "print(p)\n",
    "print(mean(male$wgt))\n",
    "print(mean(bdims$wgt))\n",
    "\n",
    "### UNCOMMENT THIS LINE TO READ DOCUMENTATION on pnorm, dnorm, etc.\n",
    "# help(pnorm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation:\n",
    "\n",
    "Correlation determines the level of association between two variables. A scatter plot among the variables is one of the ways to find correlation between variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(bdims$wgt, bdims$hgt, xlab = 'weight', ylab = 'height')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a positive correlation among height and weight variables. R has a built in function to measure the correlation. Lets use the function to verify that height and weight variables are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cor.test(bdims$wgt, bdims$hgt, method = 'pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the scatter plot suggests, correlation function supports our assumption that height and weight are associated. The level of corelation is 0.71. You can perform test on other variables in the dataset and similarly find association among other variables. Variables that are highly correlated, doesn't add much information to the model. You can also drop these variables from your final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions\n",
    "\n",
    "We will look into the most common distribution of data you will find in statistics, **normal distribution**. Distribution tells us how population is spread across a dimension. We will use some R plots to assess the normality of our data. We will also look into some of the functions to generate random numbers from a normal distribution.\n",
    "\n",
    "## Normal distribution:\n",
    "\n",
    "A normal distribution can have data spread in different ways i'e skewed towards a side either left or right or in many cases tends to be around a central value with no bias. Many scenarios in everyday life follow a Normal Distribution like\n",
    "heights of people, errors in measurements, marks on a test etc. We can say that the data is normally distributed. \n",
    "\n",
    "A normally distributed data satisfies following \n",
    "    * mean = median = mode\n",
    "    * symmetry about the center\n",
    "    * 50% of values less than the mean and 50% greater than the mean\n",
    "\n",
    "There are many things you can do if you know that the distribution is normal. For example if you know standard deviation of a normal distribution, you can say that any value is:\n",
    "\n",
    "    * likely to be within 1 standard deviation (68 out of 100 should be)\n",
    "    * very likely to be within 2 standard deviations (95 out of 100 should be)\n",
    "    * almost certainly within 3 standard deviations (997 out of 1000 should be)\n",
    "\n",
    "\n",
    "For example, 95% of students at a certain school are between 1.1m and 1.7m tall. With how much confidence can you tell that one of your friend is 1.85 meters tall. \n",
    "\n",
    "<img src='../images/normal-distribution.gif'/>\n",
    "    \n",
    "    \n",
    "    Mean is in the middle (1.1 + 1.7)/2 = 1.4 \n",
    "    \n",
    "    standard deviation is half of 95% on either side of the mean (a total of 4 standard deviations) so: (1.7-1.1)/4 = 0.15\n",
    "    \n",
    "    So where does the friend's height exactly stands in the distribution? We have to calculate z-score to find that.\n",
    "    \n",
    "            z = (Friend's height - mean height)/standard deviation\n",
    "    \n",
    "            z-score : 1.85-1.4/0.15 = 3\n",
    "            \n",
    "            z-score of 3 implies a confidence level of 99.9%.  So with a confidence of 99% you can say your friend is 1.85 meters tall\n",
    "\n",
    "\n",
    "----\n",
    "_There are 3 variety of measures to understand a distribution, _\n",
    "    * Measure of Central tendency\n",
    "    * Measure of dispersion\n",
    "    * Measure to describe shape of curve\n",
    "\n",
    "#### Measures of Central tendency:\n",
    "\n",
    "Mean, median and mode are the measures of central tendencies. They help you describe a population, through a single metric. For example if you want to compare the average salary of a professor with other professions you will compare their mean salaries.\n",
    "\n",
    "<img src='../images/central_tendency.gif'/>\n",
    "\n",
    "Of all the three measures, mean is the one which is most affected by Outliers followed by the median and mode.\n",
    "\n",
    "img source: https://www.analyticsvidhya.com/blog/2014/07/statistics/\n",
    "\n",
    "#### Measures of Dispersion:\n",
    "\n",
    "Range, Quartiles, Inter quartile range, variance and standard deviation are the measures of dispersion. They tell us how is the population distributed around the measures of central tendency. The image below shows two distributions with different standard deviations but same mean,median and mode.\n",
    "\n",
    "<img src='../images/standard_deviation.png'/>\n",
    "\n",
    "img source: wiki\n",
    "\n",
    "#### Measures to describe shape of distribution:\n",
    "\n",
    "**Skewness** – Skewness is used to measure the asymmetries of a distribution. Negatively skewed distribution has a long left tail and vice versa.\n",
    "\n",
    "**Kurtosis** – Kurtosis is used to measure the height of the “peak” of the curve. Distributions with higher peaks have positive kurtosis and vice-versa\n",
    "\n",
    "<img src='../images/skewness-and-kurtosis.gif'/>\n",
    "img source: https://www.analyticsvidhya.com/blog/2014/07/statistics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When describing the data, we can plot a normal distribution curve on top of a histogram to see how closely the data follow a normal distribution. We will work with women’s heights to illustrate this. Lets calculate some basic statistics and store them. we will use them soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "female_hgt_mean <- mean(female$hgt)\n",
    "female_hgt_sd   <- sd(female$hgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(female$hgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use density histogram as the backdrop and use the lines function to overlay a normal probability curve. The difference between a frequency histogram and a density histogram is that while in a frequency histogram the heights of the bars add up to the total number of observations, in a density histogram the areas of the bars add up to 1. \n",
    "\n",
    "Using a density histogram allows us to properly overlay a normal distribution curve over the histogram since the curve is a normal probability density function. Frequency and density histograms both display the same exact shape; they only differ in their y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist(female$hgt, probability = TRUE,main=\"female height distribution\",xlab=\"female height in cm\",ylab=\"density\")\n",
    "x <- 140:190\n",
    "y <- dnorm(x = x, mean = female_hgt_mean, sd = female_hgt_sd)\n",
    "lines(x = x, y = y, col = \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first command will plot the density histogram. We then create the x- and y-coordinates for the normal curve. x range is chosen as 140 to 190 to cover the minimum height(147.2) and maximum height(182.9). To create y, we use dnorm to calculate the density of each of those x-values in a distribution that is normal with mean female_hgt_mean and standard deviation female_hgt_sd. The last command draws a density curve on existing histogram by connecting x with corresponding values of y.\n",
    "\n",
    "The top of the curve is cut off because the limits of the x and y axes are set to best fit the histogram. To adjust the y-axis we can add a third argument to the histogram function: ylim = c(0, 0.06)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist(female$hgt, probability = TRUE,main=\"female height distribution\",xlab=\"female height in cm\",ylab=\"density\",ylim = c(0, 0.06))\n",
    "x <- 140:190\n",
    "y <- dnorm(x = x, mean = female_hgt_mean, sd = female_hgt_sd)\n",
    "lines(x = x, y = y, col = \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the normal distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we tell if the data appear to be nearly normally distributed?\n",
    "\n",
    "We can inspect the above histogram and judge it. But we cannot decide just how close the histogram is to the curve. One approach to measure the distribution of data is to construct a normal probability plot, also called a normal Q-Q plot for “quantile-quantile”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qqnorm(female$hgt)\n",
    "qqline(female$hgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data set that is nearly normal will result in a probability plot where the points closely follow the line. Any deviations from normality leads to deviations of these points from the line. The plot for female heights shows points that tend to follow the line but with some errant points towards the tails. We’re left with the same problem that we encountered with the histogram above: how close is close enough?\n",
    "\n",
    "A useful way to address this question is to rephrase it as: what do probability plots look like for data that I know came from a normal distribution? We can answer this by essentially simulating data from a normal distribution using rnorm with similar mean and standard deviation as our original female heights data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_norm <- rnorm(n = length(female$hgt), mean = female_hgt_mean, sd = female_hgt_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a normal probability plot of sim_norm and see how the points fall on the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qqnorm(sim_norm)\n",
    "qqline(sim_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the above plot compare to the probability plot for the real data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are still unsure of the answer there is even better way of comparing the original plot to compare it to many more plots using the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qqnormsim(female$hgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say the female heights are nearly normally distributed. Since we know that the data is normally distributed, we can answer all sorts of questions about that variable related to probability. \n",
    "\n",
    "\n",
    "For example, “What is the probability that a randomly chosen young adult female is taller than 6 feet (about 182 cm)?”\n",
    "\n",
    "We can find this probability by calculating a Z score and consulting a Z table (also called a normal probability table). In R, this is done in one step with the function pnorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pnorm(q = 182, mean = female_hgt_mean, sd = female_hgt_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the function pnorm gives the area under the normal curve below a given value, q, with a given mean and standard deviation. Since we’re interested in the probability that someone is taller than 182 cm, we have to take one minus that probability.\n",
    "\n",
    "<img src=\"../images/normal.PNG\"/>\n",
    "\n",
    "If we want to calculate the probability empirically, we simply need to determine how many observations fall above 182 then divide this number by the total sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(female$hgt > 182) / length(female$hgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing distributions\n",
    "\n",
    "**Note**: The following is a short guide to graphing with R. Graphs tell us a story. Data is something we cannot use directly. Because we cannot go through each and every record to understand the dataset. Instead graphs will summarize and tell us a story which can be understood by someone with little or no technical knowledge. Pictures are the best way to do this. \n",
    "\n",
    "We've already covered some exercises in graphing in the Intro to Data Science course and we will see more in the visualization course but it's good to review some basics of what we'll be doing in this course.\n",
    "\n",
    "Histograms, Bar plots will help visualize univariate data. Scatter plots will help visualize bi viriate data and see the correlations between variables. \n",
    "\n",
    "A bar plot and a pie chart are ideal if the data is categorical. This is how it works in case of a bar plot. A frequency table is generated from data which is then graphed as a bar plot. Data -> frequency table  -> bar graph\n",
    "\n",
    "|no of cylinders|frequency|\n",
    "|:-------------:|:-------:|\n",
    "|3|4|\n",
    "|4|204|\n",
    "|5|3|\n",
    "|6|84|\n",
    "|8|103|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a pie chart\n",
    "\n",
    "As mentioned above, pie charts are great when the variable is categorical. Each category is represented by a slice of the pie. Our bdims dataset has just one categorical variable, gender. The area of each slice,i'e each category in the variable is proportional to the percentage of responses in that category. \n",
    "\n",
    "Pie charts are effective for displaying the relative frequencies of a small number of categories. They are not recommended, however, when you have a large number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slices <- table(bdims$sex)\n",
    "lbls <- c(\"men\",\"women\")\n",
    "pct <- round(slices/sum(slices)*100)\n",
    "lbls <- paste(lbls, pct) # add percents to labels \n",
    "lbls <- paste(lbls,\"%\",sep=\"\") # ad % to labels \n",
    "pie(slices, labels = lbls, main=\"Pie Chart of Gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "barplot(table(auto_mpg$cylinders), main=\"Car Distribution\",xlab=\"Number of Cylinders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Histograms are better way of visualizing the data if it is continous. Data is grouped into bins. The hist() function will choose an ideal bin value automatically if you dont supply one. But choosing your own bin value will help you understand the data better. Some times a small bin size will unearth interesting patterns some times large bin sizes are helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#par(mfrow()) help you create multi-paneled plotting window. mfrow() takes an input vector of length 2. The first argument \n",
    "#specifies the number of rows and the second the number of columns of plots.\n",
    "par(mfrow=c(1,3))\n",
    "#Default bin size is 25\n",
    "hist(auto_mpg$acceleration, main=\"Acceleration with default bin size\")\n",
    "#breaks define number of elements you want in each bar. In other words, its the custom bin size\n",
    "hist(auto_mpg$acceleration, main=\"Acceleration with bin size 20\",breaks=20)\n",
    "hist(auto_mpg$acceleration, main=\"Acceleration with bin size 50\",breaks=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot(auto_mpg$mpg,data=auto_mpg, main=\"Car Milage\", xlab=\"Auto-mpg dataset\", ylab=\"Miles Per Gallon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(reference to the stats bootcamp on box plots) Recall that this box plot shows the min, 1st quartile, median, 3rd quartile, max, and outliers. The lowest bar is the min, the second lowest bar shows the 1st quartile (the cutoff point for the lowest 25% of all values), the thick middle line is the median (the cutoff point for the lowest 50%), the forth line is the 3rd quartile (the cutoff point for the lowest 75% of the values), and the highest horizontal line is the max. The circles represent the outliers. We can plot multiple boxplots from the auto_mpg data based on a second variable, say like number of cylinders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot(auto_mpg$mpg~auto_mpg$cylinders,data=auto_mpg, main=\"Car Milage\", xlab=\"Number of Cylinders\", ylab=\"Miles Per Gallon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the box plot**: The `auto` dataset has many variables. Two of them are \"miles per gallon\" and \"number of cylinders\". Here we break up the data by the number of cylinders and make a box plot for each category (3 cylinders, 4 cylinders, etc.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the number of cars in each cylinder category by using a box plot. We have to count all the cars in each category first and then plot them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate data\n",
    "\n",
    "\n",
    "Bivariate data considers two variables simultaneously. For instance, below we can plot weight versus horsepower. \n",
    "\n",
    "X-Y, scatter plots are a great way to visually explore bivariate relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot is the basic command to plot a scatter plot. It takes x and y arguements for input variables. the parameter pch is used \n",
    "#to set the symbol used to denote the observations on the plot. Here pch=19 indicates solid circles. \n",
    "plot(auto_mpg$weight, auto_mpg$horsepower, main=\"Weight of car vs Horsepower\", \n",
    "  \txlab=\"Car Weight \", ylab=\"Horse power \", pch=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common graph, that we'll see more often in the linear regression section, is the scatterplot. It just plots individual ordered pairs on a graph. The following is a scatterplot comparing the weight of the car and miles per gallon in the auto_mpg data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(auto_mpg$weight, auto_mpg$mpg, main=\"MPG vs Weight of Car\", \n",
    "  \txlab=\"Car Weight (in lbs)\", ylab=\"Miles Per Gallon\", pch=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line charts are ond of the kinds of bivariate plots. Its implementation is not as easy as other types of charts. Line charts can be created using the function lines(x, y, type=) where x and y are numeric vectors of form (x,y) points to connect. the parameter `type=` takes different values as listed below for generating different kinds of points on the plot.\n",
    "\n",
    "|type|\tdescription|\n",
    "|----|-------------|\n",
    "|p   |   \tpoints|\n",
    "|l\t |   lines|\n",
    "|o\t |   overplotted points and lines|\n",
    "|b, c|\tpoints (empty if \"c\") joined by lines|\n",
    "|s, S|\tstair steps|\n",
    "|h\t |   histogram-like vertical lines|\n",
    "|n\t |   does not produce any points or lines|\n",
    "\n",
    "Lets go ahead and make a line plot with points for female heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x <- female$hgt\n",
    "y <- female$wgt\n",
    "#par() is set to plot 3 different plots in the same panel.\n",
    "par(mfrow=c(1,3))\n",
    "plot(x, y, type=\"n\", main=\"Line chart with points\") \n",
    "lines(x, y, type='p') \n",
    "plot(x, y, type=\"n\", main=\"Line chart with lines\") \n",
    "lines(x, y, type='l') \n",
    "plot(x, y, type=\"n\", main=\"chart with overplotted lines and points\") \n",
    "lines(x, y, type='o') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above doesn't make any sense because the data is random and unordered. Lets see if we get a useful plot after sorting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x <- sort(female$hgt)\n",
    "y <- female$wgt\n",
    "plot(x, y, type=\"n\", main=\"female height vs weight\") \n",
    "lines(x, y, type='l') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is still not aesthetic and readable. But lot better than the previous plot. So line charts make sense when the data is ordered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its is important to understand how a variable is distributed before using the variable to fit a model. Some of the basic summerizing metrics we will look into are measures of central tendency(mean, median, mode), measures of variability(standard deviation, variance), percentiles.\n",
    "\n",
    "\n",
    "#### central tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mean of the displacement distribution\n",
    "paste('mean: ',mean(male$hgt))\n",
    "\n",
    "#Mean of the displacement distribution\n",
    "paste(\"median: \",median(male$hgt))\n",
    "\n",
    "#R doesn't have a built in function for mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### measures of variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#standard deviation of the displacement distribution\n",
    "paste(\"standard deviation: \",sd(male$hgt))\n",
    "\n",
    "#Variance of the displacement distribution\n",
    "paste('variance: ',var(male$hgt))\n",
    "\n",
    "#Range of the displacement distribution gives lowest and highest values in distribution\n",
    "paste('Range: ',range(male$hgt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### percentiles\n",
    "\n",
    "Definition from wiki: A percentile is a measure indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value (or score) below which 20% of the observations may be found.\n",
    "\n",
    "We apply the R quantile function to compute the percentiles with the desired percentage ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quantile(male$hgt, c(.20, .40, .90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapes of Distribution - Measure of center and Measure of spread\n",
    "\n",
    "Understanding shape of a distribution is important part of intial exploratory data analysis. After getting descriptive statistics like mean, standard deviation, skewness etc. and using graphical techniques like histograms tells us which kind of probability density function to use to fit the model. Histograms provide information about multi modal behaviour, skewness and behaviour in tails. Here we are just dealing with normal distribution of data. \n",
    "\n",
    "We will generate some random samples for normal distribution using R statements and plot histograms. Consider we have a sample of size n=200 belonging from a normal population N(10,2) with mean=10 and standard deviation=2:\n",
    "\n",
    "Go through the following link if you want to explore different kinds of distributions. Following example is taken from : https://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing shape of distribution and measure of center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.norm<-rnorm(n=200,m=10,sd=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since the prob=TRUE, we will be plotting a density plot. The y-axis represents probability density of corresponding x values\n",
    "hist(x.norm,prob=TRUE,main=\"Histogram of normal distribution\")\n",
    "#curve function helps us draw the density curve on the histogram. \n",
    "curve(dnorm(x, mean(x.norm), sd(x.norm)), add=TRUE, col=\"darkblue\", lwd=2)\n",
    "\n",
    "#calculate mean, median and sd to show them on the graph\n",
    "mean_norm <- mean(x.norm)\n",
    "median_norm = median(x.norm)\n",
    "sd_norm=sd(x.norm)\n",
    "#abline() will draw a vertical line on the graph at the specified x value. \n",
    "abline(v = mean_norm, col = \"blue\", lwd = 2)\n",
    "abline(v = median_norm, col = \"red\", lwd = 2)\n",
    "abline(v = sd_norm, col = \"green\", lwd = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above histogram represents a normal distribution of data with mean lying around 10 and standard deviation of 2. The data is aggregated close to the mean or the center and is symmetric on both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paste('mean: ',mean(x.norm))\n",
    "paste('median: ',median(x.norm))\n",
    "paste('standard deviation: ',sd(x.norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left skewed data or negative skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.norm=sort(x.norm)\n",
    "#Simulating data for plotting a left skewed distribution. We are removing the higher values from right end of sorted list whose \n",
    "#mean is 10 and sd is 2.\n",
    "left_skewed=x.norm[1:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the mean and median is shifted towards left because the data is right skewed. But the mode stays right where it is(Peak of \n",
    "# the data or most occuring value in the dataset). \n",
    "\n",
    "paste('mean: ',mean(left_skewed))\n",
    "paste('median: ',median(left_skewed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can get a histogram for this left skewed data using hist()statement\n",
    "\n",
    "hist(left_skewed,prob=TRUE,main=\"Histogram of left skewed data\")\n",
    "curve(dnorm(x, mean(left_skewed), sd(left_skewed)), add=TRUE, col=\"darkblue\", lwd=2)\n",
    "mean_right <- mean(left_skewed)\n",
    "median_right = median(left_skewed)\n",
    "abline(v = mean_right, col = \"blue\", lwd = 2)\n",
    "abline(v = median_right, col = \"red\", lwd = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph has a long tail on left side. The mean and median has shifted towards left compared to normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Right skewed data or positive skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Simulating data for plotting a positive skewed distribution. We are removing the lower values from left end of sorted list whose \n",
    "#mean is 10 and sd is 2.\n",
    "right_skewed=x.norm[51:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the mean and median is shifted towards right because the data is right skewed. \n",
    "\n",
    "paste('mean: ',mean(right_skewed))\n",
    "paste('median: ',median(right_skewed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot histogram for right skewed data\n",
    "\n",
    "hist(right_skewed,prob=TRUE,main=\"Histogram of right skewed data\")\n",
    "curve(dnorm(x, mean(right_skewed), sd(right_skewed)), add=TRUE, col=\"darkblue\", lwd=2)\n",
    "mean_left <- mean(right_skewed)\n",
    "median_left = median(right_skewed)\n",
    "abline(v = mean_left, col = \"blue\", lwd = 2)\n",
    "abline(v = median_left, col = \"red\", lwd = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is normally distributed/symmetrical and not skewed in either direction, mean would be an ideal choice if we want to measure the center. But when the data is skewed, for example if its positively skewed **mean** will give over estimate of the center and if its negatively skewed **mean** will give under estimate of the center. \n",
    "\n",
    "So, to conclude if the distribution is symmetrical, the measure of center should be **mean** and if the distribution is skewed in either direction measure of center should be **median**. Median would be the measure of the center that informs center of distribution without being affected too much by the skew as the mean does.\n",
    "\n",
    "Like wise, standard deviation would be the choice of measure for spread when the data is normally distributed. BUt when the data is skewed the curve is not symmetric anymore and the $\\mu + \\sigma$ and $\\mu - \\sigma$ doesnt represent correct areas of distribution.\n",
    "\n",
    "\n",
    "The below image shows all three kinds of distributions normal, left skewed and right skewed distributions. Observe how mean shifts with skewness. \n",
    "\n",
    "Image source[http://www.assetinsights.net/Glossary/G_Positive_Skewness.html]\n",
    "\n",
    "<img src=\"../images/Curves_Left_and_Right_skewed.JPG\"/>\n",
    "\n",
    "When the data is left skewed, mean shifts towards right. so the distribution of data which is measured as ($\\mu - 3\\sigma$ , $\\mu - 2\\sigma$ , $\\mu - \\sigma$ , $\\mu + \\sigma$ , $\\mu + 2\\sigma$ , $\\mu + 3\\sigma$) will give incorrect results. Inter quantile range is the choice of measure of spread when data is skewed like this. The quantile range tells you the spread no matter how much the data is skewed.  \n",
    "\n",
    "Look at the distributions below when data is skewed and how mean, median and mode shift with skewness. \n",
    "\n",
    "<img src=\"../images/left_skewed.JPG\"/>\n",
    "\n",
    "\n",
    "<img src=\"../images/right_skewed.JPG\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inter quartile range of normally distributed data\n",
    "\n",
    "quantile(x.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inter quartile range of left_skewed data\n",
    "\n",
    "quantile(left_skewed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inter quartile range of left_skewed data\n",
    "\n",
    "quantile(right_skewed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Skewness and Kurtosis\n",
    "\n",
    "So different shapes of distributions differ in skew and/or kurtosis. For a highly-skewed distribution, the mean can vary more than twice as the median. Mathematically skewness can be calculated or using this simple equation (Pearson): \n",
    "\n",
    "<img src='../images/pearson_skew.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness\n",
    "\n",
    "Lets calculate the skewness of the **left_skewed** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# install.packages(\"moments\", repos = \"http://cran.us.r-project.org\")\n",
    "library(moments)\n",
    "skewness(left_skewed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skewness we got is -0.768687560288046. The negative value implies that the distribution of the data is slightly skewed to the left or negatively skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets calculate the skewness of the **right_skewed** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skewness(right_skewed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skewness we got is 0.773501249208023, almost perfect opposite of what we got for right skewed data. The positive value implies that the distribution of the data is slightly skewed to the right or positively skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kurtosis\n",
    "\n",
    "Kurtosis measures the steepness of the distribution. The value \"3\" is subtracted to define \"no kurtosis\" as the kurtosis of a normal distribution. Otherwise, a normal distribution would have a kurtosis of 3. Mathematically it is represented as below\n",
    "\n",
    "<img src='../images/kurtosis_formula.gif'/>\n",
    "\n",
    "* A normal distribution has kurtosis exactly 3 (excess kurtosis exactly 0). Any distribution with kurtosis ≈3 (excess ≈0) is called mesokurtic.\n",
    "\n",
    "* A distribution with kurtosis <3 (no excess kurtosis) is called platykurtic. Compared to a normal distribution, its tails are shorter and thinner, and often its central peak is lower and broader.\n",
    "\n",
    "* A distribution with kurtosis >3 (with excess kurtosis) is called leptokurtic. Compared to a normal distribution, its tails are longer and fatter, and often its central peak is higher and sharper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets calculate the steepness of the **right_skewed** data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kurtosis(right_skewed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 3.15363009295929 implying that the distribution of the data is leptokurtic, since the computed value is greater than 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
